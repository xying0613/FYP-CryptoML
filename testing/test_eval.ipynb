{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from google.colab import drive\r\n",
    "drive.mount('/content/gdrive/')"
   ],
   "outputs": [],
   "metadata": {
    "id": "jzu6t5EKZGyQ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MODIFY HERE TO YOUR PATH IN GOOGLE DRIVE\r\n",
    "%cd /content/gdrive/MyDrive/CryptoML"
   ],
   "outputs": [],
   "metadata": {
    "id": "JpeTW4zT-XfM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install shap"
   ],
   "outputs": [],
   "metadata": {
    "id": "EdjcptLg-wwM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import speck as sp\r\n",
    "import numpy as np\r\n",
    "from tensorflow.keras.models import load_model\r\n",
    "\r\n",
    "def create_testing_data(num_of_plaintexts, num_rounds):\r\n",
    "    assert num_of_plaintexts in [2, 4]\r\n",
    "    assert num_rounds > 0\r\n",
    "\r\n",
    "    if num_of_plaintexts == 2:\r\n",
    "        X, Y = sp.make_train_data_2pt(10**6, num_rounds)\r\n",
    "        Xr, Yr = sp.real_differences_data_2pt(10**6, num_rounds)\r\n",
    "\r\n",
    "    else:\r\n",
    "        X, Y = sp.make_train_data_4pt(10**6, num_rounds)\r\n",
    "        Xr, Yr = sp.real_differences_data_4pt(10**6, num_rounds)\r\n",
    "\r\n",
    "    return X, Y, Xr, Yr\r\n",
    "\r\n",
    "def get_inputs(num_of_plaintexts, num_rounds, num_of_samples, model_name):\r\n",
    "\r\n",
    "    assert num_of_plaintexts in [2,4]\r\n",
    "    assert num_rounds > 0\r\n",
    "    assert num_of_samples > 1\r\n",
    "\r\n",
    "    num_of_plaintexts = num_of_plaintexts\r\n",
    "    num_rounds = num_rounds\r\n",
    "    net = load_model('./models/' + model_name)\r\n",
    "    num_of_samples = num_of_samples\r\n",
    "    return num_of_plaintexts, num_rounds, num_of_samples, net\r\n",
    "\r\n",
    "\r\n",
    "def evaluate(net,X,Y):\r\n",
    "    Z = net.predict(X,batch_size=10000).flatten()\r\n",
    "    Zbin = (Z > 0.5)\r\n",
    "    diff = Y - Z \r\n",
    "    mse = np.mean(diff*diff)\r\n",
    "    n = len(Z) \r\n",
    "    n0 = np.sum(Y==0) \r\n",
    "    n1 = np.sum(Y==1)\r\n",
    "    acc = np.sum(Zbin == Y) / n\r\n",
    "    tpr = np.sum(Zbin[Y==1]) / n1\r\n",
    "    tnr = np.sum(Zbin[Y==0] == 0) / n0\r\n",
    "    mreal = np.median(Z[Y==1])\r\n",
    "    high_random = np.sum(Z[Y==0] > mreal) / n0\r\n",
    "    print(\"Accuracy: \", acc, \"TPR: \", tpr, \"TNR: \", tnr, \"MSE:\", mse)\r\n",
    "    print(\"Percentage of random pairs with score higher than median of real pairs:\", 100 * high_random)"
   ],
   "outputs": [],
   "metadata": {
    "id": "OAtM9RY1-mqc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import shap\r\n",
    "\r\n",
    "# MODIFY HERE\r\n",
    "num_of_plaintexts, num_rounds, num_of_samples, net = get_inputs(num_of_plaintexts=4, num_rounds=5, num_of_samples=10, model_name=\"best5depth10.h5\")\r\n",
    "\r\n",
    "X, Y, Xr, Yr = create_testing_data(num_of_plaintexts, num_rounds)"
   ],
   "outputs": [],
   "metadata": {
    "id": "hgzPd6ZA-4XT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Testing neural distinguishers against blocks in the ordinary real vs random setting')\r\n",
    "evaluate(net, X, Y)\r\n",
    "\r\n",
    "shap.initjs()\r\n",
    "explainer = shap.KernelExplainer(net.predict, X[:num_of_samples])\r\n",
    "shap_values = explainer.shap_values(X[:num_of_samples])\r\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X)"
   ],
   "outputs": [],
   "metadata": {
    "id": "3q8kuQO4_Bzi"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Testing real differences setting now.')\r\n",
    "evaluate(net, Xr, Yr)\r\n",
    "\r\n",
    "shap.initjs()\r\n",
    "explainer = shap.KernelExplainer(net.predict, Xr[:num_of_samples])\r\n",
    "shap_values = explainer.shap_values(Xr[:num_of_samples])\r\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], Xr)"
   ],
   "outputs": [],
   "metadata": {
    "id": "o6hoseLt_DF-"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# TEST\r\n",
    "import unittest\r\n",
    "\r\n",
    "class TestEvalIPYNB(unittest.TestCase):\r\n",
    "    \r\n",
    "    def test_create_testing_data(self):\r\n",
    "        # Invalid num_of_plaintexts\r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            create_testing_data(num_of_plaintexts=0, num_rounds=5)\r\n",
    "            \r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            create_testing_data(num_of_plaintexts=10, num_rounds=5)\r\n",
    "            \r\n",
    "        # Invalid num_rounds\r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            create_testing_data(num_of_plaintexts=2, num_rounds=0)\r\n",
    "\r\n",
    "        # Valid num_of_plaintexts and num_rounds\r\n",
    "        self.assertTrue(create_testing_data(num_of_plaintexts=2, num_rounds=1))\r\n",
    "        self.assertTrue(create_testing_data(num_of_plaintexts=4, num_rounds=5))\r\n",
    "        \r\n",
    "    def test_get_inputs(self):\r\n",
    "        \r\n",
    "        dir = './models/'\r\n",
    "        \r\n",
    "        # Invalid num_of_plaintexts\r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            get_inputs(num_of_plaintexts=1, num_rounds=5, num_of_samples=10, directory=dir+\"best5depth10.h5\")\r\n",
    "            \r\n",
    "        # Invalid num_rounds\r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            get_inputs(num_of_plaintexts=2, num_rounds=0, num_of_samples=10, directory=dir+\"best5depth10.h5\")\r\n",
    "            \r\n",
    "        # Invalid num_of_samples\r\n",
    "        with self.assertRaises(AssertionError):\r\n",
    "            get_inputs(num_of_plaintexts=2, num_rounds=5, num_of_samples=1, directory=dir+\"best5depth10.h5\")\r\n",
    "            \r\n",
    "        # Invalid directory\r\n",
    "        with self.assertRaises(OSError):\r\n",
    "            get_inputs(num_of_plaintexts=4, num_rounds=5, num_of_samples=10, directory=dir+\"doesnotexist.h5\")\r\n",
    "            \r\n",
    "        self.assertTrue(get_inputs(num_of_plaintexts=4, num_rounds=5, num_of_samples=10, directory=dir+\"best5depth10.h5\"))\r\n",
    "            \r\n",
    "    def test_evaluate(self):\r\n",
    "        \r\n",
    "        X, Y, Xr, Yr = create_testing_data(4, 5)\r\n",
    "        net = load_model(dir+\"best5depth10.h5\")\r\n",
    "        \r\n",
    "        # Invalid model\r\n",
    "        with self.assertRaises(AttributeError):\r\n",
    "            evaluate('notmodel', X, Y)\r\n",
    "        \r\n",
    "        # Invalid data X and Y\r\n",
    "        with self.assertRaises(IndexError):\r\n",
    "            evaluate(net, 1, 1)\r\n",
    "                            \r\n",
    "        X, Y, Xr, Yr = create_testing_data(2, 5)\r\n",
    "        with self.assertRaises(ValueError):\r\n",
    "            evaluate(net, X, Y)\r\n",
    "        \r\n",
    "if __name__ == '__main__': \r\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False, verbosity=2)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eval.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}